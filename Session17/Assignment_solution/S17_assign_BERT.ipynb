{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9LK26bYx7jl",
        "outputId": "e87a4de3-f4a9-4970-ffda-31b5b8d0b64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install torchinfo\n",
        "\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/shreyash-99/ERA1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZdRLhd3yC-5",
        "outputId": "0c677cb5-7340-4ed7-e1c4-aa5dea8d5b0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERA1'...\n",
            "remote: Enumerating objects: 368, done.\u001b[K\n",
            "remote: Counting objects: 100% (368/368), done.\u001b[K\n",
            "remote: Compressing objects: 100% (357/357), done.\u001b[K\n",
            "remote: Total 368 (delta 9), reused 366 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (368/368), 19.48 MiB | 23.44 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ERA1/Session17/Assignment_solution/transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy_E40GhyEJA",
        "outputId": "4d0ff910-302d-47ee-c643-7249d316b786"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ERA1/Session17/Assignment_solution/transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer import BERT\n",
        "from bert_helper import SentencesDataset"
      ],
      "metadata": {
        "id": "7iB5UwCTyFuO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdWquTvvyX10",
        "outputId": "8a3f9220-8fab-4b1d-c17c-e726d5d9b3d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../../SessionCodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQsVoufqzBKK",
        "outputId": "5bd9a7fd-d0b7-46e9-c922-5c6560a5e604"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ERA1/Session17/SessionCodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = 'BERT/training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vie9XdXXya87",
        "outputId": "a0f6ec36-8059-4d9e-ec58-44ab481b3a00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = BERT(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSAkM3g6ycOl",
        "outputId": "a68edb14-7239-417f-e669-b3735899146a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n",
            "initializing optimizer and loss...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../Assignment_solution/transformer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF4tTZ6yzZpS",
        "outputId": "88bfd009-008f-4f21-d8b9-e812fb7c8bd7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ERA1/Session17/Assignment_solution/transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_helper import get_batch"
      ],
      "metadata": {
        "id": "aiFk-4Wfzqxi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 1000\n",
        "for it in range(n_iteration):\n",
        "\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPgl1FigydlB",
        "outputId": "39bf50f2-b774-4eac-e86d-c85c53d27e86"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "it: 0  | loss 10.2  | Δw: 1.136\n",
            "it: 10  | loss 9.55  | Δw: 0.558\n",
            "it: 20  | loss 9.38  | Δw: 0.326\n",
            "it: 30  | loss 9.16  | Δw: 0.248\n",
            "it: 40  | loss 9.04  | Δw: 0.208\n",
            "it: 50  | loss 8.84  | Δw: 0.184\n",
            "it: 60  | loss 8.69  | Δw: 0.17\n",
            "it: 70  | loss 8.56  | Δw: 0.158\n",
            "it: 80  | loss 8.31  | Δw: 0.146\n",
            "it: 90  | loss 8.22  | Δw: 0.144\n",
            "it: 100  | loss 8.04  | Δw: 0.125\n",
            "it: 110  | loss 7.96  | Δw: 0.116\n",
            "it: 120  | loss 7.75  | Δw: 0.116\n",
            "it: 130  | loss 7.65  | Δw: 0.109\n",
            "it: 140  | loss 7.54  | Δw: 0.113\n",
            "it: 150  | loss 7.43  | Δw: 0.104\n",
            "it: 160  | loss 7.35  | Δw: 0.094\n",
            "it: 170  | loss 7.25  | Δw: 0.091\n",
            "it: 180  | loss 7.08  | Δw: 0.087\n",
            "it: 190  | loss 7.01  | Δw: 0.093\n",
            "it: 200  | loss 6.96  | Δw: 0.087\n",
            "it: 210  | loss 6.9  | Δw: 0.089\n",
            "it: 220  | loss 6.87  | Δw: 0.083\n",
            "it: 230  | loss 6.71  | Δw: 0.082\n",
            "it: 240  | loss 6.67  | Δw: 0.084\n",
            "it: 250  | loss 6.65  | Δw: 0.081\n",
            "it: 260  | loss 6.67  | Δw: 0.08\n",
            "it: 270  | loss 6.66  | Δw: 0.079\n",
            "it: 280  | loss 6.57  | Δw: 0.083\n",
            "it: 290  | loss 6.51  | Δw: 0.083\n",
            "it: 300  | loss 6.45  | Δw: 0.081\n",
            "it: 310  | loss 6.54  | Δw: 0.08\n",
            "it: 320  | loss 6.41  | Δw: 0.084\n",
            "it: 330  | loss 6.4  | Δw: 0.085\n",
            "it: 340  | loss 6.44  | Δw: 0.09\n",
            "it: 350  | loss 6.41  | Δw: 0.085\n",
            "it: 360  | loss 6.39  | Δw: 0.087\n",
            "it: 370  | loss 6.37  | Δw: 0.091\n",
            "it: 380  | loss 6.44  | Δw: 0.102\n",
            "it: 390  | loss 6.4  | Δw: 0.1\n",
            "it: 400  | loss 6.39  | Δw: 0.099\n",
            "it: 410  | loss 6.35  | Δw: 0.118\n",
            "it: 420  | loss 6.38  | Δw: 0.108\n",
            "it: 430  | loss 6.32  | Δw: 0.115\n",
            "it: 440  | loss 6.4  | Δw: 0.11\n",
            "it: 450  | loss 6.34  | Δw: 0.114\n",
            "it: 460  | loss 6.33  | Δw: 0.117\n",
            "it: 470  | loss 6.41  | Δw: 0.125\n",
            "it: 480  | loss 6.38  | Δw: 0.132\n",
            "it: 490  | loss 6.41  | Δw: 0.141\n",
            "it: 500  | loss 6.32  | Δw: 0.154\n",
            "it: 510  | loss 6.36  | Δw: 0.151\n",
            "it: 520  | loss 6.33  | Δw: 0.163\n",
            "it: 530  | loss 6.37  | Δw: 0.201\n",
            "it: 540  | loss 6.36  | Δw: 0.181\n",
            "it: 550  | loss 6.33  | Δw: 0.188\n",
            "it: 560  | loss 6.3  | Δw: 0.226\n",
            "it: 570  | loss 6.37  | Δw: 0.223\n",
            "it: 580  | loss 6.3  | Δw: 0.219\n",
            "it: 590  | loss 6.38  | Δw: 0.228\n",
            "it: 600  | loss 6.31  | Δw: 0.258\n",
            "it: 610  | loss 6.36  | Δw: 0.277\n",
            "it: 620  | loss 6.25  | Δw: 0.292\n",
            "it: 630  | loss 6.3  | Δw: 0.317\n",
            "it: 640  | loss 6.3  | Δw: 0.326\n",
            "it: 650  | loss 6.31  | Δw: 0.346\n",
            "it: 660  | loss 6.28  | Δw: 0.367\n",
            "it: 670  | loss 6.3  | Δw: 0.376\n",
            "it: 680  | loss 6.25  | Δw: 0.433\n",
            "it: 690  | loss 6.26  | Δw: 0.415\n",
            "it: 700  | loss 6.3  | Δw: 0.442\n",
            "it: 710  | loss 6.37  | Δw: 0.434\n",
            "it: 720  | loss 6.27  | Δw: 0.478\n",
            "it: 730  | loss 6.24  | Δw: 0.496\n",
            "it: 740  | loss 6.24  | Δw: 0.532\n",
            "it: 750  | loss 6.35  | Δw: 0.533\n",
            "it: 760  | loss 6.33  | Δw: 0.531\n",
            "it: 770  | loss 6.28  | Δw: 0.554\n",
            "it: 780  | loss 6.27  | Δw: 0.602\n",
            "it: 790  | loss 6.36  | Δw: 0.651\n",
            "it: 800  | loss 6.2  | Δw: 0.624\n",
            "it: 810  | loss 6.28  | Δw: 0.607\n",
            "it: 820  | loss 6.3  | Δw: 0.63\n",
            "it: 830  | loss 6.15  | Δw: 0.674\n",
            "it: 840  | loss 6.28  | Δw: 0.65\n",
            "it: 850  | loss 6.26  | Δw: 0.664\n",
            "it: 860  | loss 6.27  | Δw: 0.675\n",
            "it: 870  | loss 6.18  | Δw: 0.675\n",
            "it: 880  | loss 6.17  | Δw: 0.744\n",
            "it: 890  | loss 6.26  | Δw: 0.745\n",
            "it: 900  | loss 6.24  | Δw: 0.756\n",
            "it: 910  | loss 6.15  | Δw: 0.771\n",
            "it: 920  | loss 6.25  | Δw: 0.941\n",
            "it: 930  | loss 6.25  | Δw: 0.828\n",
            "it: 940  | loss 6.21  | Δw: 0.863\n",
            "it: 950  | loss 6.21  | Δw: 0.887\n",
            "it: 960  | loss 6.25  | Δw: 0.903\n",
            "it: 970  | loss 6.17  | Δw: 0.892\n",
            "it: 980  | loss 6.19  | Δw: 0.95\n",
            "it: 990  | loss 6.16  | Δw: 0.943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4WRj7xwyfJt",
        "outputId": "b0d1416d-4419-453b-d90f-a2edde66976c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving embeddings...\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftubO85Ryrk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4zexlCviytd8"
      }
    }
  ]
}
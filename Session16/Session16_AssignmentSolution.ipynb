{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "model.py\n"
      ],
      "metadata": {
        "id": "ct87dTHCpakp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import MultiheadAttention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "## coding our own LAYER NORMALISZATION CODE as the inbuilt one doesnt allow bias = false\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps:float = 10**-6)-> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))    ## alpha and beta are learnable parameters\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_length , hidden_size]\n",
        "        mean = x.mean(-1, keepdim = True) # [batch, seq, 1]\n",
        "        std = x.std(-1, keepdim = True) # [batch , seq , 1]\n",
        "        #keep the dimension for broadcasting , if (keepdim = False) - the last dimension will not be there - [batch , seqdim]\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "\n",
        "## FEED FORWARD NETWORK - using squeeze and expand method\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model : int, d_ff : int, dropout:float)-> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model , d_ff) ## w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## [batch, seq_length, d_model] -> [batch, seq_length, d_ff] -> [batch, seq_length, d_model]\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "## for converting inputs to dimensional embedding prepared to go in encoder or decoder.\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model:int, vocab_size:int)-> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [batch_size, seq_length] -> [batch_size, seq_length, d_model]\n",
        "        # multiply by sqrt(d_model) to scale the embedding according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model:int, seq_len:int, dropout:float )-> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # create a matrix of shape (seq_len , d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # create a vector of shape [seq_len]\n",
        "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) ## [seq_len , 1]\n",
        "        # create a vector of shape [d_model]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [d_model]\n",
        "        # apply sine to even indices\n",
        "        pe[ : , 0::2] = torch.sin(position * div_term) # sin(position * (10000**(2i/d_model))\n",
        "        # apply cosine to all odd indices\n",
        "        pe[ : , 1::2] = torch.cos(position * div_term) # cos(position * (10000**(2i/d_model))\n",
        "        # add a batch to positional encoding\n",
        "        pe = pe.unsqueeze(0)\n",
        "        ## register the positional encoding as BUFFER(non trainable)\n",
        "        self.register_buffer('pe' , pe) ## saves the value of pe as \"pe\" even if the kernel gets closed, and this is not back_propagated\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, : x.shape[1] , :]).requires_grad_(False) # [batch, seq_len , d_model]\n",
        "        # x = x + (self.pe[:, : , :]).requires_grad_(False)\n",
        "        # x.shape[1] gives the seq_length of a sentence.\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout:float)-> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x))) ## cant understand it currently\n",
        "\n",
        "\n",
        "## MULTI HEAD ATTENTION part which we can use for BOTH ENCODER and DECODER\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model:int, h:int, dropout:float)-> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        #make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # dimension of embedding seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias = False) #Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias = False) #Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias = False) #Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias = False) #Wo\n",
        "        ## Heads are not considered yet in the above code\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod  # we can directly use call function without instantiating the multi head attention class by using: MultiHeadAtetntion.attention(...)\n",
        "    def attention(query, key, value, mask, dropout:nn.Dropout):\n",
        "        d_k = query.shape[-1] # gives the last dimension which is the dimension_size of each head i.e. d_k\n",
        "        # Just apply formula from the paper\n",
        "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            ## write a very low value(indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e4)\n",
        "        # applying softmax along the values of last dimension(could have been applied along any of last 2 dimensions, doesnt matter)\n",
        "        attention_scores = attention_scores.softmax(dim = -1) # [batch, h, seq_length, seq_length]\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        ## (batch, h, seq_length, seq_length) -> (batch, h , seq_length, d_k)\n",
        "        # return attention scores which can be used for visualisation\n",
        "        return (attention_scores @ value) , attention_scores\n",
        "\n",
        "    def forward(self, q, k , v , mask):\n",
        "        query = self.w_q(q) ## [batch, seq_length, d_model] -> [batch, seq_length, d_model]\n",
        "        key = self.w_k(k) # [batch, seq_length, d_model] -> [batch, seq_length, d_model]\n",
        "        value = self.w_v(v) # [batch, seq_length, d_model] -> [batch, seq_length, d_model]\n",
        "\n",
        "        # dividing it into h heads\n",
        "        #[batch, seq_length, d_model] -> [batch, seq_length, h, d_k] -> [batch, h, seq_length, d_k]\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # query.shape[1] = seq_length(), query.shape[0] = batch\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        #calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        #combine all heads together\n",
        "        # (batch, h, seq_length, d_k) -> (batch, seq_length, h , d_k) - > (batch , seq_length, d_model)\n",
        "        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h * self.d_k)\n",
        "\n",
        "        # multipply by wo\n",
        "        # (batch , seq_length, d_model) -> (batch , seq_length, d_model)\n",
        "        return self.w_o(x)\n",
        "\n",
        "## a single encoder block\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block:MultiHeadAttentionBlock, feed_forward_block:FeedForwardBlock , dropout:float)-> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x, src_mask))\n",
        "        ## as for an encoder key, query, value have same inputs\n",
        "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
        "        # in encoder block, one can see 2 skip connections, one before and after the MHA and one before after the Feed forward layer.\n",
        "        return x\n",
        "\n",
        "## Actual encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block:MultiHeadAttentionBlock, cross_attention_block : MultiHeadAttentionBlock, feed_forward_block:FeedForwardBlock, dropout:float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x ,x, x, tgt_mask))\n",
        "        # initial masked multi head attention layer where encoder outputs are not used\n",
        "        x = self.residual_connection[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        # cross attention layers where query is x, and key and value are from encoder blocks\n",
        "        x = self.residual_connection[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "        # final feed forward layer\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers : nn.ModuleList)-> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layers in self.layers:\n",
        "            x = layers( x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "## for converting the final enmbedding to the vocabulary space meaning which work is most likely to come\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x)-> None:\n",
        "        # [batch, seq_length, d_model] -> [batch, seq_length, vocab_size]\n",
        "        return torch.log_softmax(self.proj(x), dim = -1)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed : InputEmbeddings, tgt_embed : InputEmbeddings, src_pos : PositionalEncoding, tgt_pos : PositionalEncoding, projection_layer : ProjectionLayer  )-> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        #[batch, seq_length, d_model]\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor )-> None:\n",
        "        # [batch, seq_length, d_model]\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        # target - the thing we need to predict\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "        return decoder_output\n",
        "\n",
        "    def project(self, x):\n",
        "        # [batch, seq_length, vocab_size]\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_length: int, tgt_seq_length: int, d_model: int = 512, N:int=6, h:int=8, dropout:float = 0.1, d_ff:int=256):\n",
        "    # create embedding layer\n",
        "\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # create positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_length, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_length, dropout)\n",
        "\n",
        "    # create encoder blocks\n",
        "    encoder_blocks = []\n",
        "    # N - no of encoder and decoder blocks\n",
        "    for _ in range(N // 2):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "\n",
        "    # create decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N // 2):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout )\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    e1, e2, e3 = encoder_blocks\n",
        "    d1, d2, d3 = decoder_blocks\n",
        "\n",
        "    encoder_blocks1 = [e1, e2, e3, e3, e2, e1]\n",
        "    decoder_blocks1 = [d1, d2, d3, d3, d2, d1]\n",
        "\n",
        "\n",
        "    # create the encoder and decoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks1))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks1))\n",
        "\n",
        "\n",
        "\n",
        "    #create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # initialise the parameters(will work even if we dont do this)\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.normal_(p, std = 0.02)\n",
        "\n",
        "    n_param = sum(p.numel() for p in transformer.parameters())\n",
        "    print(\"Total Parameters:\", n_param)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "P0F5-LUDpoHI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "config.py\n"
      ],
      "metadata": {
        "id": "i2-5beQupvw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return{\n",
        "        \"batch_size\":2048,\n",
        "        \"num_epochs\":20,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\":130,\n",
        "        \"d_model\" : 512,\n",
        "        \"lang_src\" : \"en\",\n",
        "        \"lang_tgt\" : \"fr\",\n",
        "        \"model_folder\" : \"weights\",\n",
        "        \"model_basename\" : \"tmodel_\",\n",
        "        \"preload\" : True,\n",
        "        \"tokenizer_file\" : \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\" : \"runs/tmodel\",\n",
        "        \"scheduler\": None\n",
        "\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config[\"model_folder\"]\n",
        "    model_basename = config[\"model_basename\"]\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.')/ model_folder / model_filename)"
      ],
      "metadata": {
        "id": "Yf4n5UoKpoYc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset.py\n"
      ],
      "metadata": {
        "id": "GNa8Y8uPppR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "## convert from one language to another\n",
        "class BillingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype = torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype = torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype = torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ## extracting the text fromt he input\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        ## transform text into token\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        ##add sos eos and padding to each of the sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 ## add both sos and eod\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 ## add only sos and not eos\n",
        "\n",
        "        ## make sure number of padding tokenn is not negative. If it is, sentence is too long\n",
        "        if enc_num_padding_tokens < 0  or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence too long\")\n",
        "\n",
        "        ## add sos and eos token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype = torch.int64),\n",
        "                self.eos_token,\n",
        "                # torch.tensor([self.pad_token] * enc_num_padding_tokens , dtype = torch.int64),\n",
        "            ],\n",
        "            dim = 0,\n",
        "        )\n",
        "\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64),\n",
        "                # torch.tensor([self.pad_token] * dec_num_padding_tokens , dtype = torch.int64),\n",
        "            ],\n",
        "            dim = 0,\n",
        "        )\n",
        "\n",
        "        ## add only eos token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64),\n",
        "                self.eos_token,\n",
        "                # torch.tensor([self.pad_token] * dec_num_padding_tokens , dtype = torch.int64),\n",
        "            ],\n",
        "            dim = 0,\n",
        "        )\n",
        "\n",
        "        ## NOTICE THE DIFFERENCE b/w DECODER_INPUT and LABEL, this difference allows us to parallely train decoder models\n",
        "        ## for any index i, input is from 0 to i of decoder input and label(or prediction) is ith of label which is actually the next word.\n",
        "\n",
        "        # double check the size of tensors to make sure they are fo same length i.e. seq_len\n",
        "\n",
        "        # assert encoder_input.size(0) == self.seq_len\n",
        "        # assert decoder_input.size(0) == self.seq_len\n",
        "        # assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\" : encoder_input,\n",
        "            \"decoder_input\" : decoder_input,\n",
        "            \"encoder_str_length\":len(enc_input_tokens),\n",
        "            \"decoder_str_length\":len(dec_input_tokens),\n",
        "            \"encoder_mask\" : (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  ## (1,1,seq_len)\n",
        "            # where ever encoder token is not equal to pad token, pass TRUE, and where it is equal to pad pass FALSE , thereforE of type(T, T ,T, F, F, F, F)\n",
        "            \"decoder_mask\" : (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1,seq_len) & (1, seq_len, seq_len)\n",
        "            ## seq_len = 10\n",
        "            ## SOS    I  GOT   A   CAT    PAD    PAD    PAD    PAD    PAD    PAD\n",
        "            ## TRUE TRUE TRUE TRUE TRUE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE\n",
        "            ## 1 1 1 1 1 0 0 0 0 0\n",
        "            ## Upper triangular matrix\n",
        "            ## 1 1 1 1 1 1 1 1 1 1\n",
        "            ## 0 1 1 1 1 1 1 1 1 1\n",
        "            ## 0 0 1 1 1 1 1 1 1 1\n",
        "            ## 0 0 0 1 1 1 1 1 1 1\n",
        "            ## 0 0 0 0 1 1 1 1 1 1\n",
        "            ## 0 0 0 0 0 1 1 1 1 1\n",
        "            ## 0 0 0 0 0 0 1 1 1 1\n",
        "            ## 0 0 0 0 0 0 0 1 1 1\n",
        "            ## 0 0 0 0 0 0 0 0 1 1\n",
        "            ## 0 0 0 0 0 0 0 0 0 1\n",
        "\n",
        "            ## after AND operation - Final Decoder Mask\n",
        "            ## 1 1 1 1 1 0 0 0 0 0\n",
        "            ## 0 1 1 1 1 0 0 0 0 0\n",
        "            ## 0 0 1 1 1 0 0 0 0 0\n",
        "            ## 0 0 0 1 1 0 0 0 0 0\n",
        "            ## 0 0 0 0 1 0 0 0 0 0\n",
        "            ## 0 0 0 0 0 0 0 0 0 0\n",
        "            ## 0 0 0 0 0 0 0 0 0 0\n",
        "            ## 0 0 0 0 0 0 0 0 0 0\n",
        "            ## 0 0 0 0 0 0 0 0 0 0\n",
        "            ## 0 0 0 0 0 0 0 0 0 0\n",
        "            \"label\" : label, #(seq_len)\n",
        "            \"src_text\" : src_text,\n",
        "            \"tgt_text\" : tgt_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    ## creates upper traigular matrix of ones with diagonal = 1.\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal = 1).type(torch.int)\n",
        "    return mask == 0\n"
      ],
      "metadata": {
        "id": "IDhGxU8Ppoo6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.py\n"
      ],
      "metadata": {
        "id": "9Gah1kVFpam-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from model import build_transformer\n",
        "# from dataset import BillingualDataset, causal_mask\n",
        "# from config import get_config, get_weights_file_path\n",
        "\n",
        "\n",
        "!pip install torchtext\n",
        "!pip3 install datasets\n",
        "!pip3 install tokenizers\n",
        "\n",
        "from torchtext import datasets as datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# hugging face datasets and tokenizer\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "!pip3 install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "## basically speeds up some part of code\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    ## precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    ## initialise decoder input with sos token\n",
        "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len: # if we reach max length before getting the eos token\n",
        "            break\n",
        "\n",
        "        ## build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim = 1,\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples= 2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "\n",
        "    except:\n",
        "        # if we cant get the console width\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device) # [B, seq_len]\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # [B, 1, 1, seq_len]\n",
        "\n",
        "            ## check that the batch size is 1\n",
        "            assert encoder_input.size(0) == 1, \"batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            ## print the sourc , target and model output\n",
        "\n",
        "            print_msg('-'* console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "    if writer:\n",
        "        ## evaluate the character error rate\n",
        "        ## compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation_cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        ## compute word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation_wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        ## compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        ## most code taken from  https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = \"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"], min_frequency = 2)\n",
        "        ## for a word to be a part of our dataset, it should atleast come twice otherwise its not the part of our dataset\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "\n",
        "# def train_model(config):\n",
        "#     ## define the device\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(\"Using device: \", device)\n",
        "\n",
        "#     ## make sure the weights folder exists\n",
        "#     Path(config['model_folder']).mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "#     train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "#     model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "#     # tensorboard\n",
        "#     writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'] , eps = 1e-9)\n",
        "    # ## each feature can have different learnign rate, so for words seen less it can increase learning rate of those weights\n",
        "\n",
        "    # ## if the user has specified a model to preload before training , load it\n",
        "\n",
        "    # initial_epoch = 0\n",
        "    # global_step = 0\n",
        "    # if config['preload']:\n",
        "    #     model_filename = get_weights_file_path(config, config['preload'])\n",
        "    #     print(f'Preloading model {model_filename}')\n",
        "    #     state = torch.load(model_filename)\n",
        "    #     model.load_state_dict(state['model_state_dict'])\n",
        "    #     initial_epoch = state['epoch'] + 1\n",
        "    #     optimizer.load_state_dict(state['optimizer_state_dict'])       ## important to store optimiser for Adam as all weights have different lr\n",
        "    #     global_step = state['global_step']\n",
        "    #     print(\"preloaded\")\n",
        "\n",
        "    # loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing= 0.1)\n",
        "\n",
        "    # for epoch in range(initial_epoch, config['num_epochs']):\n",
        "    #     torch.cuda.empty_cache()\n",
        "    #     model.train()\n",
        "    #     batch_iterator = tqdm(train_dataloader, desc = f\"Processing Epoch {epoch:02d}\")\n",
        "\n",
        "    #     for batch in batch_iterator:\n",
        "\n",
        "    #         encoder_input = batch['encoder_input'].to(device) # [B, seq_len]\n",
        "    #         decoder_input = batch['decoder_input'].to(device) # [B, seq_len]\n",
        "    #         encoder_mask = batch['encoder_mask'].to(device) # [B, 1, 1, Seq_len]\n",
        "    #         decoder_mask = batch['decoder_mask'].to(device) # [B, 1, Seq_len, Seq_len]\n",
        "\n",
        "    #         ## run the tensors through the encoder, decoder and projection layer\n",
        "    #         encoder_output = model.encode(encoder_input, encoder_mask)  # [B, seq_len, d_model]\n",
        "    #         decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "    #         proj_output = model.project(decoder_output) # [B, seq_len, Vocab_size]\n",
        "\n",
        "    #         ## compare the ouput with the label\n",
        "    #         label = batch['label'].to(device) ## [B, seq_len]\n",
        "\n",
        "    #         ## compute the loss using simple cross entropy\n",
        "    #         loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "    #         batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "    #         ## log the loss\n",
        "    #         writer.add_scalar('train_loss', loss.item(), global_step)\n",
        "    #         writer.flush()\n",
        "\n",
        "    #         ## backpropagate the loss\n",
        "    #         loss.backward()\n",
        "\n",
        "    #         ## update the weights\n",
        "    #         optimizer.step()\n",
        "    #         optimizer.zero_grad(set_to_none = True)\n",
        "\n",
        "    #         global_step += 1\n",
        "\n",
        "    #         # break\n",
        "\n",
        "    #     ## run validation at the end of every epoch\n",
        "    #     run_validation(model,val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg : batch_iterator.write(msg) , global_step, writer)\n",
        "\n",
        "    #     if epoch > 0:\n",
        "    #         prev_model_filename  = get_weights_file_path(config, f\"{epoch - 1:02d}\")\n",
        "    #         os.remove(prev_model_filename)\n",
        "\n",
        "    #     model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "    #     torch.save({\n",
        "    #         'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #         'global_step': global_step\n",
        "    #    }, model_filename )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW9ECHPWp6yH",
        "outputId": "03daae6b-339b-472b-f6ad-f673d34d94ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.0.4)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds(config):\n",
        "    # it has only the train split, so we divide it ourselves\n",
        "    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split = 'train')\n",
        "    print(\"dataset_size\" , len(ds_raw))\n",
        "\n",
        "    # build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ## keep 90% for traning and 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "    sorted_train_ds = sorted(train_ds_raw, key = lambda x:len(x[\"translation\"][config['lang_src']]))\n",
        "    # sorted_train_ds = train_ds_raw ## not sorted, taken as it is\n",
        "    filtered_sorted_train_ds = [k for k in sorted_train_ds if (len(k['translation'][config['lang_src']]) < 150 and  len(k['translation'][config['lang_src']]) > 3)]\n",
        "    filtered_sorted_train_ds = [k for k in filtered_sorted_train_ds if (len(k['translation'][config['lang_tgt']]) < 150 and len(k['translation'][config['lang_tgt']]) > 3)]\n",
        "    filtered_sorted_train_ds = [k for k in filtered_sorted_train_ds if len(k['translation'][config['lang_src']]) + 10 > len(k['translation'][config['lang_tgt']]) ]\n",
        "\n",
        "\n",
        "    train_ds = BillingualDataset(filtered_sorted_train_ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    # train_ds = BillingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BillingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "\n",
        "    # find max length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "\n",
        "    max_len_src_filtered = 0\n",
        "    max_len_tgt_filtered = 0\n",
        "    for item in filtered_sorted_train_ds:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src_filtered = max(max_len_src_filtered, len(src_ids))\n",
        "        max_len_tgt_filtered = max(max_len_tgt_filtered, len(tgt_ids))\n",
        "\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    print(f'Max length of filtered source sentence: {max_len_src_filtered}')\n",
        "    print(f'Max length of filterd target sentence: {max_len_tgt_filtered}')\n",
        "\n",
        "    print(\"length of train dataset\" , len(train_ds))\n",
        "    print(\"length of validation dataset\" , len(val_ds))\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True, collate_fn = collate_fn )\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    encoder_input_max = max(b['encoder_str_length'] for b in batch)\n",
        "    decoder_input_max = max(b['decoder_str_length'] for b in batch)\n",
        "    encoder_input_max += 2\n",
        "    decoder_input_max += 2\n",
        "\n",
        "    # input_size_max = max(encoder_input_max, decoder_input_max)\n",
        "\n",
        "    pad_token_encoder = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype = torch.int64)\n",
        "    pad_token_decoder = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype = torch.int64)\n",
        "\n",
        "    encoder_inputs = []\n",
        "    decoder_inputs = []\n",
        "    encoder_masks = []\n",
        "    decoder_masks = []\n",
        "    labels = []\n",
        "    src_texts = []\n",
        "    tgt_texts = []\n",
        "\n",
        "    for b in batch:\n",
        "        enc_num_padding_token = encoder_input_max - len(b['encoder_input'])\n",
        "        dec_num_padding_token = decoder_input_max - len(b['decoder_input'])\n",
        "        label_num_padding_token = decoder_input_max - len(b['label'])\n",
        "\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                b['encoder_input'],\n",
        "                torch.tensor([pad_token_encoder] * enc_num_padding_token , dtype = torch.int64)\n",
        "            ],\n",
        "            dim = 0,\n",
        "        )\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                b['decoder_input'],\n",
        "                torch.tensor([pad_token_decoder] * dec_num_padding_token, dtype = torch.int64)\n",
        "            ],\n",
        "            dim = 0,\n",
        "        )\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                b['label'],\n",
        "                torch.tensor([pad_token_decoder] * label_num_padding_token, dtype = torch.int64)\n",
        "            ],\n",
        "            dim = 0,\n",
        "        )\n",
        "        encoder_mask = (encoder_input != pad_token_encoder).unsqueeze(0).unsqueeze(0).int()\n",
        "        decoder_mask = (decoder_input != pad_token_decoder).unsqueeze(0).int() & causal_mask(decoder_input_max)\n",
        "        encoder_inputs.append(encoder_input)\n",
        "        decoder_inputs.append(decoder_input)\n",
        "        encoder_masks.append(encoder_mask)\n",
        "        decoder_masks.append(decoder_mask)\n",
        "        labels.append(label)\n",
        "        src_texts.append(b[\"src_text\"])\n",
        "        tgt_texts.append(b['tgt_text'])\n",
        "\n",
        "    # print(k.size() for k in encoder_inputs)\n",
        "    # print(k.shape() for k in decoder_inputs)\n",
        "    # print(k.shape() for k in encoder_masks)\n",
        "    # print(k.shape() for k in decoder_masks)\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": torch.vstack(encoder_inputs),\n",
        "        \"decoder_input\": torch.vstack(decoder_inputs),\n",
        "        \"encoder_mask\": torch.vstack(encoder_masks),\n",
        "        \"decoder_mask\": torch.vstack(decoder_masks),\n",
        "        \"label\" : torch.vstack(labels),\n",
        "        \"src_text\" : src_texts,\n",
        "        \"tgt_text\": tgt_texts\n",
        "    }\n",
        "\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], d_model = config['d_model'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "1VVE8jnHFIWS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from config import get_config\n",
        "\n",
        "cfg = get_config()\n",
        "cfg['batch_size'] = 8\n",
        "cfg['preload'] = None\n",
        "cfg['num_epochs'] = 10\n",
        "\n",
        "# from train import train_model\n",
        "\n",
        "torch.cuda.amp.autocast(enabled=True)\n",
        "## in pytorch lightening, check if the above command is already enabled when precision is set to FP16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)\n",
        "\n",
        "## make sure the weights folder exists\n",
        "Path(cfg['model_folder']).mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(cfg)\n",
        "\n",
        "\n",
        "model = get_model(cfg, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "# tensorboard\n",
        "writer = SummaryWriter(cfg['experiment_name'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = cfg['lr'] , eps = 1e-9)\n",
        "## each feature can have different learnign rate, so for words seen less it can increase learning rate of those weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbsUdsgQp7pP",
        "outputId": "9c0068e8-0a34-4483-c7bd-87fe0187002a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda\n",
            "dataset_size 127085\n",
            "Max length of source sentence: 471\n",
            "Max length of target sentence: 482\n",
            "Max length of filtered source sentence: 45\n",
            "Max length of filterd target sentence: 48\n",
            "length of train dataset 60888\n",
            "length of validation dataset 12709\n",
            "Total Parameters: 57124690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "metadata": {
        "id": "x3_as-uJoU2M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LR = 10**-3\n",
        "STEPS_PER_EPOCH = len(train_dataloader)\n",
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "Uro61tsNnrag"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                max_lr = MAX_LR,\n",
        "                                                steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                                                epochs = EPOCHS,\n",
        "                                                pct_start = 1/10 if EPOCHS != 1 else 0.5,\n",
        "                                                div_factor = 10,\n",
        "                                                three_phase = True,\n",
        "                                                final_div_factor = 10,\n",
        "                                                anneal_strategy = \"linear\"\n",
        "                                                )"
      ],
      "metadata": {
        "id": "p5dtcCViooT3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## if the user has specified a model to preload before training , load it\n",
        "initial_epoch = 0\n",
        "global_step = 0\n",
        "# if config['preload']:\n",
        "#     model_filename = get_weights_file_path(config, config['preload'])\n",
        "#     print(f'Preloading model {model_filename}')\n",
        "#     state = torch.load(model_filename)\n",
        "#     model.load_state_dict(state['model_state_dict'])\n",
        "#     initial_epoch = state['epoch'] + 1\n",
        "#     optimizer.load_state_dict(state['optimizer_state_dict'])       ## important to store optimiser for Adam as all weights have different lr\n",
        "#     global_step = state['global_step']\n",
        "#     print(\"preloaded\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing= 0.1)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "lr = [0.0]\n",
        "\n",
        "for epoch in range(initial_epoch, EPOCHS):\n",
        "    loss_acc = []\n",
        "\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc = f\"Processing Epoch {epoch:02d}\")\n",
        "\n",
        "    for batch in batch_iterator:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        encoder_input = batch['encoder_input'].to(device) # [B, seq_len]\n",
        "        decoder_input = batch['decoder_input'].to(device) # [B, seq_len]\n",
        "        encoder_mask = batch['encoder_mask'].to(device) # [B, 1, 1, Seq_len]\n",
        "        decoder_mask = batch['decoder_mask'].to(device) # [B, 1, Seq_len, Seq_len]\n",
        "\n",
        "        ## run the tensors through the encoder, decoder and projection layer\n",
        "        # print(encoder_input.shape)\n",
        "        # print(decoder_mask.shape)\n",
        "        encoder_mask = encoder_mask.unsqueeze(1)\n",
        "        decoder_mask = decoder_mask.unsqueeze(1)\n",
        "        # print(encoder_mask.shape)\n",
        "\n",
        "        with torch.autocast(device_type = 'cuda', dtype = torch.float16 ):\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)  # [B, seq_len, d_model]\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output) # [B, seq_len, Vocab_size]\n",
        "\n",
        "            ## compare the ouput with the label\n",
        "            label = batch['label'].to(device) ## [B, seq_len]\n",
        "\n",
        "            ## compute the loss using simple cross entropy\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            loss_acc.append(loss)\n",
        "            batch_iterator.set_postfix(\n",
        "                {\"loss_acc\": f\"{torch.mean(torch.stack(loss_acc)).item():6.3f}\",\n",
        "                    \"loss\": f\"{loss.item():6.3f}\", \"lr\" : f\"{get_lr(optimizer)}\"\n",
        "                })\n",
        "\n",
        "\n",
        "\n",
        "        ## log the loss\n",
        "        writer.add_scalar('train_loss', loss.item(), global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        ## backpropagate the loss\n",
        "        # loss.backward()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        ## update the weights\n",
        "        # optimizer.step()\n",
        "        scale = scaler.get_scale()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        skip_lr_sched = (scale > scaler.get_scale())\n",
        "        if not skip_lr_sched:\n",
        "            scheduler.step()\n",
        "        lr.append(scheduler.get_last_lr())\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "\n",
        "    ## run validation at the end of every epoch\n",
        "    run_validation(model,val_dataloader, tokenizer_src, tokenizer_tgt, cfg['seq_len'], device, lambda msg : batch_iterator.write(msg) , global_step, writer)\n",
        "\n",
        "    ## remove the prev model files\n",
        "    if epoch > 0:\n",
        "        prev_model_filename  = get_weights_file_path(cfg, f\"{epoch - 1:02d}\")\n",
        "        os.remove(prev_model_filename)\n",
        "\n",
        "    model_filename = get_weights_file_path(cfg, f\"{epoch:02d}\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'global_step': global_step\n",
        "    }, model_filename )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwVQGIQhnhs2",
        "outputId": "ae9aa217-b8da-462a-e74a-516d35f7e1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|| 7611/7611 [17:14<00:00,  7.36it/s, loss_acc=4.823, loss=4.476, lr=0.00039989488437281004]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Unable to comprehend the captain's resistance, he hastened to say to him,\n",
            "    TARGET: Ne pouvant sexpliquer la rsistance du capitaine, il se hta de lui dire :\n",
            " PREDICTED: Le capitaine voulut lui comprendre le capitaine , il se hta de lui dire :\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The elder one, whom you have seen (and whom I cannot hate, whilst I abhor all his kindred, because he has some grains of affection in his feeble mind, shown in the continued interest he takes in his wretched sister, and also in a dog-like attachment he once bore me), will probably be in the same state one day.\n",
            "    TARGET: L'an, que vous avez vu (et que je ne puis pas har, bien que je dteste toute sa famille, parce que cet esprit faible a montr, par son continuel intrt pour sa malheureuse soeur, qu'il y avait en lui quelque peu d'affection, et parce qu'autrefois il a eu pour moi un attachement de chien), aura probablement, un jour  venir, le mme sort que les autres;\n",
            " PREDICTED: Le bon , que vous m  avez vu , et que je ne puis pas , je ne le , car il me semble que de faire en faire l  esprit .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
            "  _future_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
            "  _future_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `BLEUScore` from `torchmetrics` was deprecated and will be removed in 2.0. Import `BLEUScore` from `torchmetrics.text` instead.\n",
            "  _future_warning(\n",
            "Processing Epoch 01: 100%|| 7611/7611 [19:14<00:00,  6.59it/s, loss_acc=3.801, loss=3.257, lr=0.0006997897687456201]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE:  Si cependant vous avez la bont de me mettre au courant de vos investigations, continua-t-il, je serai heureux de vous preter mon concours dans la limite de mes moyens.\n",
            "    TARGET: \"If you will let me know how your investigations go,\" he continued, \"I shall be happy to give you any help I can.\n",
            " PREDICTED: \" If you have done my of my , I ' s , I ,\" he continued , \" I am , \" I am in my of my .\"\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"No doubt it is a geyser, like those in Iceland.\"\n",
            "    TARGET: --Eh! sans doute, geyser, riposte mon oncle, un geyser pareil  ceux de l'Islande!\n",
            " PREDICTED: -- Non , c ' est un homme , comme ceux dans l ' Islande .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 02: 100%|| 7611/7611 [19:10<00:00,  6.62it/s, loss_acc=3.460, loss=3.657, lr=0.000999645234758234]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The fair Amanda reflected for a while.\n",
            "    TARGET: La belle Amanda rflchit un peu.\n",
            " PREDICTED: L  Amanda songea pour un temps .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: But his thoughts had never turned in that direction, and, moreover, he had not the least inclination for riotous living.\n",
            "    TARGET: Il n'y avait pas pens, parce que sa chair tait morte, et qu'il ne se sentait plus le moindre apptit de dbauche.\n",
            " PREDICTED: Mais sa pense n ' avait pas dans cette direction , et , et , il n ' avait pas le moins de .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 03: 100%|| 7611/7611 [19:06<00:00,  6.64it/s, loss_acc=3.113, loss=3.385, lr=0.0007004598808689559]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: A noise aroused him; someone was knocking at the door, trying to open it.\n",
            "    TARGET: Un bruit le rveilla, on frappait a la porte, on essayait d'ouvrir.\n",
            " PREDICTED: Un bruit le fit ; quelqu  un s  en allait , en s  en .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: When they reached the shop, everyone was ready: Grivet and Olivier, the witnesses of Therese, were there, along with Suzanne, who looked at the bride as little girls look at dolls they have just dressed up.\n",
            "    TARGET: Lorsqu'ils arrivrent  la boutique, tout le monde tait prt: il y avait l Grivet et Olivier, tmoins de Thrse, et Suzanne qui regardait la marie comme les petites filles regardent les poupes qu'elles viennent d'habiller.\n",
            " PREDICTED: Quand ils furent prts  la boutique ; chacun tait prt  Olivier et Olivier , la tmoins , il y avait des filles , qui regardait de l ' avoir des .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 04: 100%|| 7611/7611 [19:48<00:00,  6.40it/s, loss_acc=2.633, loss=3.261, lr=0.00040060441485634203]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE:  Voila mon dernier anneau, cria-t-il, tout est complet maintenant. \n",
            "    TARGET: \"The last link,\" he cried, exultantly. \"My case is complete.\"\n",
            " PREDICTED: \" That last the last ring ,\" he cried , \" is his astonishment .\"\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: CHAPTER 16\n",
            "    TARGET: CHAPITRE XVI.\n",
            " PREDICTED: CHAPITRE XVI\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 05: 100%|| 7611/7611 [19:55<00:00,  6.37it/s, loss_acc=2.229, loss=2.265, lr=0.0001007489488437281]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: If your feelings are still what they were last April, tell me so at once. _My_ affections and wishes are unchanged, but one word from you will silence me on this subject for ever.\"\n",
            "    TARGET: Les miens nont pas vari, non plus que le reve que javais form alors. Mais un mot de vous suffira pour mimposer silence a jamais.\n",
            " PREDICTED: Si vos sentiments sont encore si elles me le dire , si je suis incapable de me rendre la route , mais un mot de silence pour vous , je vous rponds sur ce sujet .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"So did I, madam, and I am excessively disappointed.\n",
            "    TARGET: -- Moi aussi, madame, et vous me voyez trs dsappoint.\n",
            " PREDICTED: -- Je m ' a dit , madame , et je suis un peu longue .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 06: 100%|| 7611/7611 [19:41<00:00,  6.44it/s, loss_acc=1.992, loss=2.046, lr=9.626086004434347e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Observing Conseil, I discovered that, just barely, the gallant lad had fallen under the general influence.\n",
            "    TARGET: En observant Conseil, je constatai que ce brave garon subissait tant soit peu l'influence gnrale.\n",
            " PREDICTED: Je , Conseil ,  peine ,  peine , que le brave garon tait tomb sous l ' influence gnral .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"I tell you that she can't see you.\"\n",
            "    TARGET: -- Je n'y peux rien, s'cria la femme d'un ton rude, je vous rpte qu'elle ne peut vous voir.\n",
            " PREDICTED: -- Je vous dis qu ' elle ne peut pas vous voir .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 07: 100%|| 7611/7611 [19:46<00:00,  6.42it/s, loss_acc=1.936, loss=1.878, lr=9.25123586894041e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Among some unimportant papers he found the following letter, that which he had sought at the risk of his life:\n",
            "    TARGET: Au milieu de quelques papiers sans importance, il trouva la lettre suivante: c'tait celle qu'il tait all chercher au risque de sa vie:\n",
            " PREDICTED: Quelques papiers , aprs avoir trouv la lettre , ce qui tait essay de courir au risque de sa vie :\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Thereupon her son had a nervous attack, and threatened to fall ill, if she did not give way to his whim.\n",
            "    TARGET: Son fils eut une crise de nerfs, il la menaa de tomber malade, si elle ne cdait pas  son caprice.\n",
            " PREDICTED: L - dessus son fils avait une crise nerveuse , et il menaait de ne pas se laisser agir franchement , de sa fantaisie .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 08: 100%|| 7611/7611 [19:46<00:00,  6.42it/s, loss_acc=1.892, loss=1.719, lr=8.876336462923932e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Mrs. Fairfax turned out to be what she appeared, a placid-tempered, kind-natured woman, of competent education and average intelligence.\n",
            "    TARGET: Mme Fairfax tait en effet ce qu'elle m'avait paru tout d'abord, une femme douce, complaisante, suffisamment instruite, et d'une intelligence ordinaire.\n",
            " PREDICTED: Mme Fairfax se tourna vers cette ducation de l ' ducation tranquille et capables de l ' ducation de capables , de l ' intelligence et de toute intelligence .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The host drew back and burst into tears.\n",
            "    TARGET: L'hte recula d'un pas et se mit  fondre en larmes.\n",
            " PREDICTED: L ' hte se recula , et clata en sanglots .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 09: 100%|| 7611/7611 [18:47<00:00,  6.75it/s, loss_acc=1.856, loss=2.037, lr=8.501486327429995e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I thought I recognised you at street-corners, and I ran after all the carriages through whose windows I saw a shawl fluttering, a veil like yours.\"\n",
            "    TARGET: Jai cru vous reconnatre au coin des rues; et je courais aprs tous les fiacres o flottait  la portire un chle, un voile pareil au vtre...\n",
            " PREDICTED: Je croyais que vous dans les coin de la rue ; et je courus de toute la voiture des voitures au un voile , bien tourner la vtre .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: TIENNE had at last descended from the platform and entered the Voreux; he spoke to men whom he met, asking if there was work to be had, but all shook their heads, telling him to wait for the captain.\n",
            "    TARGET: tienne, descendu enfin du terri, venait d'entrer au Voreux; et les hommes auxquels il s'adressait, demandant s'il y avait du travail, hochaient la tete, lui disaient tous d'attendre le matre-porion.\n",
            " PREDICTED: Des descendirent sur la plate - forme et , il rentra au Voreux ; il a des hommes qu ' il rencontrait , s ' il n ' y avait que la tete .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 10: 100%|| 7611/7611 [18:49<00:00,  6.74it/s, loss_acc=1.826, loss=1.917, lr=8.126636191936059e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The porter said, 'Yes, madam'; and the constable began not to like it, and would have persuaded the mercer to dismiss him, and let me go, since, as he said, he owned I was not the person.\n",
            "    TARGET: Le commissionnaire dit: Oui, madame; et la chose commena de dplaire au commissaire qui s'effora de persuader au mercier de me congdier et de me laisser aller, puisque, ainsi qu'il disait, il convenait que je n'tais point la personne.\n",
            " PREDICTED: Le portier , madame . Et le commissaire se mit  commissaire en commissaire , et ne l ' et pas dit le mercier , car je fus reue , et me , car il ne le but , car il ne pouvait .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Are you a Christian?\"\n",
            "    TARGET: -- tes-vous chrtien?\n",
            " PREDICTED: -- Vous tes un chrtien ?\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 11: 100%|| 7611/7611 [18:45<00:00,  6.76it/s, loss_acc=1.802, loss=1.787, lr=7.751835326964662e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Listen!\" he said to her; and she shuddered at the sound of that fatal voice which she had not heard for a long time.\n",
            "    TARGET:  coute , lui dit-il, et elle frmit au son de cette voix funeste quelle navait pas entendue depuis longtemps. Il continua.\n",
            " PREDICTED:  coutez , lui dit - il , et elle frissonnait au son de cette voix fatal qui ne l ' avait pas entendu pendant longtemps .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: At eight o'clock Justin came to fetch him to shut up the shop.\n",
            "    TARGET:  huit heures, Justin venait le chercher pour fermer la pharmacie.\n",
            " PREDICTED:  huit heures , Justin venait l  aller chercher pour qu  il fermait la boutique .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 12: 100%|| 7611/7611 [18:50<00:00,  6.74it/s, loss_acc=1.779, loss=1.795, lr=7.376985191470725e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"That was what I wished you to think.\"\n",
            "    TARGET:  Cest ce que je dsirais vous faire croire.\n",
            " PREDICTED: -- C ' est ce que je voulais vous croire .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Eight o'clock struck.\n",
            "    TARGET: Huit heures sonnaient.\n",
            " PREDICTED: Huit heures sonnerent .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 13: 100%|| 7611/7611 [18:41<00:00,  6.79it/s, loss_acc=1.760, loss=1.979, lr=7.002184326499329e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Yes, but water decomposed into its primitive elements,\" replied Cyrus Harding, \"and decomposed doubtless, by electricity, which will then have become a powerful and manageable force, for all great discoveries, by some inexplicable laws, appear to agree and become complete at the same time.\n",
            "    TARGET: -- Oui, mais l'eau dcompose en ses lments constitutifs, rpondit Cyrus Smith, et dcompose, sans doute, par l'lectricit, qui sera devenue alors une force puissante et maniable, car toutes les grandes dcouvertes, par une loi inexplicable, semblent concorder et se complter au mme moment.\n",
            " PREDICTED: -- Oui , mais dans ses lments Bunzen , rpondit Cyrus Smith , et il aura t par l ' lectricit qui se tout  fait pour un grand fracas .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: En effet, jai vu deux de ses espions particuliers, de moi bien connus, se promener dans ma rue jusque sur le minuit.\n",
            "    TARGET: As a matter of fact, I saw two of his private spies, well known to me, patrolling my street until nearly midnight.\n",
            " PREDICTED: I have seen , my pendant me of spies , Lestrade will be able to in the who asked .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 14: 100%|| 7611/7611 [19:03<00:00,  6.66it/s, loss_acc=1.743, loss=1.720, lr=6.627383461527933e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"By God! I tell you you shall drink a glass in here; I'll break the jaws of the first man who looks askance at me!\"\n",
            "    TARGET: Nom de Dieu! je te dis que tu vas boire une chope la-dedans, je casse la gueule au premier qui me regarde de travers!\n",
            " PREDICTED:  Nom de Dieu ! je te dis que tu vas boire un verre , moi , je les mchoires de l ' air qui me de travers .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Perhaps other creeks also ran towards the west, but they could not be seen.\n",
            "    TARGET: Peut-tre d'autres creeks couraient-ils vers l'ouest, mais rien ne permettait de le constater.\n",
            " PREDICTED: Peut - etre aussi , tant - elle aussi ah ! mais ils ne pouvaient voir de l ' ouest .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 15:  66%|   | 5004/7611 [12:10<06:26,  6.74it/s, loss_acc=1.715, loss=1.729, lr=6.38088303725399e-05] "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zde0XvvYp48J"
      }
    }
  ]
}
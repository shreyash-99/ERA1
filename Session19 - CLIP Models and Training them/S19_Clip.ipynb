{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_8leDcI4f6B",
        "outputId": "497c7c6c-e222-489d-b513-886f7f9b2012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "!pip install transformers\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDxaudk5TUrY",
        "outputId": "09656b4c-91a6-46b0-8030-d734874d6ca1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PIL.JpegImagePlugin.JpegImageFile"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "H_RmbGWsJfGF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_classifier(img, text1, text2, text3, text4):\n",
        "    # print(000000)\n",
        "    input = []\n",
        "    if(text1 != \"\"):\n",
        "        input.append(text1)\n",
        "    if(text2 != \"\"):\n",
        "        input.append(text2)\n",
        "    if(text3 != \"\"):\n",
        "        input.append(text3)\n",
        "    if(text4 != \"\"):\n",
        "        input.append(text4)\n",
        "    # print(input)\n",
        "    # print(11111111)\n",
        "    # print(type(img))\n",
        "    inputs = processor(text=input, images=img, return_tensors=\"pt\", padding=True)\n",
        "    # print(22222222)\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
        "    probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
        "    # print(probs)\n",
        "    output = {}\n",
        "    for i, text in enumerate(input):\n",
        "        output[text] = probs[0][i].item()\n",
        "    print(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "gst6I98pJ0vt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygh_pfASdRVh",
        "outputId": "81b3ec5d-9ff1-45de-f393-aaf60777e84e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=clip_classifier,\n",
        "    inputs=[\n",
        "        gr.Image(type= \"pil\", shape=(512, 512), image_mode=\"RGB\", label= \"Input Image\"),\n",
        "        gr.Textbox(lines=1, placeholder=\"Text 1...\"),\n",
        "        gr.Textbox(lines=1, placeholder=\"Text 2...\"),\n",
        "        gr.Textbox(lines=1, placeholder=\"Text 3...\"),\n",
        "        gr.Textbox(lines=1, placeholder=\"Text 4...\")],\n",
        "        outputs=gr.Label(),\n",
        "        examples = [[\"women.png\", \"women wearing a scarf\", \"man sitting on a chair\", \"snake crawling on a road\", \"black and white picture\"],\n",
        "               ],\n",
        "        description=\"OpenAI CLIP image classifier\"\n",
        ")\n",
        "demo.launch(debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "TRAzbzOYKTfv",
        "outputId": "ee92885f-94ee-4e4b-aaf7-c80dcf967109"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c81a35ad58c254d5d1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c81a35ad58c254d5d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "['women wearing a scarf', 'man sitting on a chair', 'snake crawling on a road', 'black and white picture']\n",
            "11111111\n",
            "<class 'PIL.Image.Image'>\n",
            "22222222\n",
            "tensor([[9.9105e-01, 3.4609e-05, 9.3937e-08, 8.9145e-03]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "{'women wearing a scarf': 0.9910508394241333, 'man sitting on a chair': 3.460880179773085e-05, 'snake crawling on a road': 9.393719579975368e-08, 'black and white picture': 0.008914452977478504}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oW6fEUgYQGfh"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}